# Semantic Search Engine - Mon premier projet IA avec Python

## üìã Vue d'ensemble

Application de **recherche s√©mantique** qui comprend le sens des requ√™tes plut√¥t que de faire une simple correspondance de mots-cl√©s. Par exemple, chercher "voiture rapide" trouvera aussi des documents contenant "automobile sportive" ou "v√©hicule performant".

Ce projet a √©t√© d√©velopp√© pour apprendre et comprendre comment fonctionne un moteur de recherche moderne et a par la m√™me occasion servi de projet final soumis √† Harvard pour valider mon cours CS50.

### Technologies principales
- **Backend** : Flask + FAISS + Sentence Transformers
- **Frontend** : Vue.js 3 + Vite
- **Orchestration** : Docker Compose

---

## üöÄ D√©marrage

```bash
# Cloner et lancer
git clone <repo>
cd semantic-search
docker-compose up --build

# Puis ouvrir dans votre navigateur
# Frontend : http://localhost:5173
# API : http://localhost:5000
```

---

## üß† Comment fonctionne la recherche s√©mantique ?

### Le probl√®me avec la recherche classique

Une recherche traditionnelle cherche des **correspondances exactes** :
- Requ√™te : "voiture rapide"
- Trouve : documents contenant exactement "voiture" ET "rapide"
- Rate : "automobile sportive", "v√©hicule performant", "bolide"

### La solution : les embeddings vectoriels

Au lieu de comparer des mots, on compare des **vecteurs math√©matiques** qui repr√©sentent le **sens** du texte.

```
"machine learning"          ‚Üí  [0.2, 0.8, 0.1, ..., 0.5]  (384 dimensions)
"apprentissage automatique" ‚Üí  [0.21, 0.79, 0.11, ..., 0.49]

Similarit√© cosinus √©lev√©e ‚Üí textes similaires !
```

---

## üî¨ Architecture du moteur de recherche

### √âtape 1 : Pr√©paration des donn√©es (`dataset.py`)

```python
documents = [
    "Machine learning is a subset of AI",
    "Deep learning uses neural networks",
    "Python is great for data science"
]
```

G√©n√®re :
- **`data/`** : Fichiers sources bruts
- **`docs.db`** : Base SQLite avec m√©tadonn√©es (id, titre, texte)

### √âtape 2 : Indexation (`indexer.py`)

```python
# 1. Charger le mod√®le
model = SentenceTransformer("all-MiniLM-L6-v2")

# 2. Encoder tous les documents
embeddings = model.encode(documents)
# ‚Üí Matrice de vecteurs normalis√©s pour la similarit√© cosinus

# 3. Cr√©er l'index FAISS avec similarit√© cosinus
index = faiss.IndexFlatIP(384)  # IP = Inner Product (cosinus sur vecteurs normalis√©s)

# 4. Ajouter les vecteurs
index.add(embeddings)

# 5. Sauvegarder
faiss.write_index(index, "indexes.faiss")
```

**R√©sultat :** `indexes.faiss` - Index optimis√© pour la recherche rapide

### √âtape 3 : Recherche en temps r√©el (`app.py`)

```python
# 1. Utilisateur cherche : "deep learning"
query_vector = model.encode(query)

# 2. Chercher les 5 vecteurs les plus similaires
similarities, indices = index.search(query_vector, k=5)

# similarities = [0.95, 0.87, 0.72, 0.65, 0.58]  # Plus √©lev√© = plus similaire
# indices = [42, 15, 103, 8, 67]

# 3. R√©cup√©rer les documents depuis SQLite
results = [
    {"id": 42, "title": "...", "score": 0.95},
    {"id": 15, "title": "...", "score": 0.87},
    ...
]
```

---

## üéØ Pourquoi c'est puissant ?

### 1. Comprend les synonymes
```
Requ√™te : "automobile"
Trouve : "voiture", "v√©hicule", "car"
```

### 2. Comprend le contexte
```
Requ√™te : "apple fruit"
Trouve : documents sur les pommes (fruit)
Pas : documents sur Apple (entreprise)
```

### 3. Fonctionne en multilingue
```
Requ√™te : "machine learning"
Trouve aussi : "apprentissage automatique"
```

### 4. Rapide et scalable
- **10 000 documents** : ~5-10ms
- **1 million de documents** : ~50-100ms

---

## üîç FAISS : Le c≈ìur du syst√®me

**FAISS** (Facebook AI Similarity Search) permet la recherche vectorielle ultra-rapide.

### IndexFlatIP : Similarit√© cosinus

Notre projet utilise `IndexFlatIP` qui calcule le **produit scalaire** (√©quivalent √† la similarit√© cosinus pour des vecteurs normalis√©s) :

```python
index = faiss.IndexFlatIP(384)
```

**Avantages :**
- Mesure la similarit√© directionnelle (orientation des vecteurs)
- Score entre -1 et 1 (1 = identique, 0 = orthogonal, -1 = oppos√©)
- Plus intuitif : **score √©lev√© = meilleur r√©sultat**
- Pr√©cis √† 100%

**Diff√©rence avec L2 :**
- `IndexFlatL2` : Distance euclidienne (longueur du vecteur compte)
- `IndexFlatIP` : Similarit√© cosinus (seule l'orientation compte)

Pour les embeddings textuels normalis√©s, la similarit√© cosinus est g√©n√©ralement pr√©f√©r√©e.

---

## üìä Sentence Transformers : Le cerveau

### Le mod√®le : all-MiniLM-L6-v2

**Caract√©ristiques :**
- **Dimension** : 384
- **Taille** : ~80 MB
- **Vitesse** : ~14 000 phrases/seconde (CPU)
- **Qualit√©** : Excellent √©quilibre performance/pr√©cision

```python
model = SentenceTransformer("all-MiniLM-L6-v2")

model.encode("chat")  # ‚Üí [0.12, -0.54, ...]
model.encode("J'aime les chats")  # ‚Üí [0.15, -0.52, ...]
```

Le mod√®le a √©t√© entra√Æn√© sur des millions de paires de phrases pour capturer le sens s√©mantique.

---

## üé® Flux complet d'une recherche

```
1. Utilisateur : "python data science"
   ‚Üì
2. Frontend ‚Üí GET /search?q=python data science
   ‚Üì
3. Flask encode la requ√™te
   ‚Üí [0.23, 0.67, -0.12, ..., 0.89]
   ‚Üì
4. FAISS calcule les similarit√©s cosinus
   ‚Üí [doc_42: 0.89, doc_15: 0.82, doc_103: 0.75]
   ‚Üì
5. SQLite r√©cup√®re les m√©tadonn√©es
   ‚Üì
6. JSON ‚Üí Frontend
   [{id: 42, title: "...", score: 0.89}, ...]
```

---

## üß™ Exemple concret

### Documents index√©s :
```
[1] "Python est un langage de programmation"
[2] "JavaScript est utilis√© pour le web"
[3] "Le machine learning utilise Python"
[4] "Les pandas mangent du bambou"
```

### Requ√™te : "programmation python"

**Similarit√©s cosinus calcul√©es :**
```
doc[1] : 0.92  ‚Üê Tr√®s similaire !
doc[2] : 0.35
doc[3] : 0.78  ‚Üê Assez similaire
doc[4] : 0.12  ‚Üê Peu similaire
```

**R√©sultats (tri√©s par score d√©croissant) :**
```json
[
  {
    "id": 1,
    "title": "Python est un langage de programmation",
    "score": 0.92
  },
  {
    "id": 3,
    "title": "Le machine learning utilise Python",
    "score": 0.78
  }
]
```

---

## üîß API Endpoints

### `GET /`
Informations sur l'index

```json
{
  "message": "FAISS API ready",
  "index_size": 1000,
  "dimension": 384
}
```

### `GET /search?q=votre requ√™te`
Recherche s√©mantique

**R√©ponse :**
```json
[
  {
    "id": 42,
    "title": "Introduction to Machine Learning",
    "snippet": "Machine learning is a subset...",
    "score": 0.89
  }
]
```

**Note :** Avec `IndexFlatIP`, **plus le score est √©lev√©, meilleur est le r√©sultat** (contrairement √† L2 o√π un score bas est meilleur).

---

## üìñ Ressources

- [FAISS](https://faiss.ai/) - Biblioth√®que de recherche vectorielle
- [Sentence Transformers](https://www.sbert.net/) - Mod√®les d'embeddings
- [Flask](https://flask.palletsprojects.com/) - Framework web Python
- [Hugging Face Models](https://huggingface.co/models?library=sentence-transformers) - Mod√®les alternatifs

---

**Le principe :** Transformer du texte en vecteurs, calculer leur similarit√© cosinus pour trouver ce qui est s√©mantiquement proche. Simple et puissant ! üöÄ