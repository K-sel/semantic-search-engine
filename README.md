# Semantic Search Engine - Mon premier projet IA avec Python

## üìã Vue d'ensemble

Application de **recherche s√©mantique** qui comprend le sens des requ√™tes plut√¥t que de faire une simple correspondance de mots-cl√©s. Par exemple, chercher "voiture rapide" trouvera aussi des documents contenant "automobile sportive" ou "v√©hicule performant".

Ce projet a √©t√© d√©velopp√© pour apprendre et comprendre comment fonctionne un moteur de recherche moderne et a par la m√™me occasion servi de projet final soumis √† Harvard pour valider mon cours CS50.

### Technologies principales
- **Backend** : Flask + FAISS + Sentence Transformers
- **Frontend** : Vue.js 3 + Vite
- **Orchestration** : Docker Compose

---

## üöÄ D√©marrage

```bash
# Cloner et lancer
git clone <repo>
cd semantic-search
docker-compose up --build

# Puis ouvrir dans votre navigateur
# Frontend : http://localhost:5173
# API : http://localhost:5000 ou depuis frontend http://localhost:5173/api/search?q=machine
```

---

## üß† Comment fonctionne la recherche s√©mantique ?

### Le probl√®me avec la recherche classique

Une recherche traditionnelle (comme CTRL+F) cherche des **correspondances exactes** :
- Requ√™te : "voiture rapide"
- Trouve : documents contenant exactement "voiture" ET "rapide"
- Rate : "automobile sportive", "v√©hicule performant", "bolide"

### La solution : les embeddings vectoriels

Au lieu de comparer des mots, on compare des **vecteurs math√©matiques** qui repr√©sentent le **sens** du texte.

```
"machine learning"     ‚Üí  [0.2, 0.8, 0.1, ..., 0.5]  (384 dimensions)
"apprentissage automatique" ‚Üí  [0.21, 0.79, 0.11, ..., 0.49]

Distance entre les vecteurs = faible ‚Üí textes similaires !
```

---

## üìö Cas d'usage

### 1. Documentation technique
Chercher dans une base de docs techniques : "comment g√©rer les erreurs" trouve aussi "exception handling" et "error management"

### 2. E-commerce
"chaussures de course" trouve aussi "baskets running", "sneakers sport"

### 3. Support client
"mon compte ne marche pas" trouve des articles sur "probl√®mes de connexion", "erreurs d'authentification"

### 4. Recherche acad√©mique
Trouver des papers similaires par le contenu plut√¥t que par mots-cl√©s exacts

---

## üî¨ Architecture du moteur de recherche

### √âtape 1 : Pr√©paration des donn√©es (`dataset.py`)

```python
# Vos documents sources
documents = [
    "Machine learning is a subset of AI",
    "Deep learning uses neural networks",
    "Python is great for data science"
]

# Sauvegarde dans data/ et SQLite
```

Le script g√©n√®re :
- **`data/`** : Fichiers sources bruts
- **`docs.db`** : Base SQLite avec les m√©tadonn√©es (id, titre, texte)

### √âtape 2 : Indexation (`indexer.py`)

**Le processus :**

```python
# 1. Charger le mod√®le de transformation texte ‚Üí vecteur
model = SentenceTransformer("all-MiniLM-L6-v2")

# 2. Pour chaque document
for doc in documents:
    # Transformer le texte en vecteur de 384 dimensions
    embedding = model.encode(doc)
    # ‚Üí [0.12, -0.43, 0.89, ..., 0.15]

# 3. Cr√©er l'index FAISS
index = faiss.IndexFlatL2(384)  # 384 = dimension des vecteurs

# 4. Ajouter tous les vecteurs √† l'index
index.add(all_embeddings)

# 5. Sauvegarder l'index sur disque
faiss.write_index(index, "indexes.faiss")
```

**R√©sultat :**
- **`indexes.faiss`** : Index optimis√© pour la recherche rapide

### √âtape 3 : Recherche en temps r√©el (`app.py`)

Quand un utilisateur cherche quelque chose :

```python
# 1. L'utilisateur tape : "deep learning"
query = "deep learning"

# 2. Transformer la requ√™te en vecteur avec le M√äME mod√®le
query_vector = model.encode(query)
# ‚Üí [0.18, 0.72, -0.31, ..., 0.44]

# 3. Chercher les 5 vecteurs les plus proches dans l'index
distances, indices = index.search(query_vector, k=5)

# distances = [0.12, 0.45, 0.89, 1.23, 1.87]  # Plus petit = plus similaire
# indices = [42, 15, 103, 8, 67]  # IDs des documents

# 4. R√©cup√©rer les documents depuis SQLite
for doc_id in indices:
    doc = db.execute("SELECT * FROM docs WHERE id = ?", [doc_id])
    
# 5. Retourner les r√©sultats avec leur score
results = [
    {"id": 42, "title": "...", "snippet": "...", "score": 0.12},
    {"id": 15, "title": "...", "snippet": "...", "score": 0.45},
    ...
]
```

---

## üéØ Pourquoi c'est puissant ?

### 1. Comprend les synonymes

```
Requ√™te : "automobile"
Trouve : documents sur "voiture", "v√©hicule", "car"
```

### 2. Comprend le contexte

```
Requ√™te : "apple fruit"
Trouve : documents sur les pommes (fruit)
Ne trouve PAS : documents sur Apple (entreprise)
```

### 3. Fonctionne en multilingue

Le mod√®le `all-MiniLM-L6-v2` comprend plusieurs langues :
```
Requ√™te en anglais : "machine learning"
Trouve aussi : documents fran√ßais sur "apprentissage automatique"
```

### 4. Rapide et scalable

FAISS utilise des algorithmes optimis√©s :
- **10 000 documents** : recherche en ~5-10ms
- **1 million de documents** : recherche en ~50-100ms

---

## üîç FAISS : Le c≈ìur du syst√®me

**FAISS** (Facebook AI Similarity Search) est une biblioth√®que pour la recherche de similarit√© vectorielle ultra-rapide.

### Comment √ßa marche ?

Au lieu de comparer votre requ√™te avec **tous** les documents un par un (lent), FAISS utilise des structures de donn√©es optimis√©es :

**Approche na√Øve (lente) :**
```python
# Comparer avec chaque document
for doc_vector in all_docs:
    distance = calculate_distance(query_vector, doc_vector)
# Complexit√© : O(n) ‚Üí lent pour des millions de docs
```

**Approche FAISS (rapide) :**
```python
# Index organis√© intelligemment (comme un arbre)
distances, indices = index.search(query_vector, k=5)
# Complexit√© : O(log n) ou mieux
```

### Types d'index FAISS

Notre projet utilise `IndexFlatL2` (recherche exacte) :
- Pr√©cis √† 100%
- Rapide jusqu'√† ~1 million de vecteurs
- Distance : L2 (euclidienne)

Pour des datasets √©normes, FAISS propose :
- **IndexIVFFlat** : Approximation rapide
- **IndexHNSW** : Graphes hi√©rarchiques
- **IndexPQ** : Compression des vecteurs

---

## üìä Sentence Transformers : Le cerveau

**Sentence Transformers** transforme du texte en vecteurs qui capturent le sens s√©mantique.

### Le mod√®le : all-MiniLM-L6-v2

**Caract√©ristiques :**
- **Dimension** : 384 (chaque texte ‚Üí vecteur de 384 nombres)
- **Taille** : ~80 MB
- **Vitesse** : ~14 000 phrases/seconde sur CPU
- **Qualit√©** : Excellent √©quilibre performance/pr√©cision

**Comment il fonctionne :**
```python
model = SentenceTransformer("all-MiniLM-L6-v2")

# Un mot
model.encode("chat")  # ‚Üí [0.12, -0.54, ...]

# Une phrase
model.encode("J'aime les chats")  # ‚Üí [0.15, -0.52, ...]

# Un paragraphe entier
model.encode("Les chats sont des animaux...")  # ‚Üí [0.14, -0.53, ...]
```

Le mod√®le a √©t√© entra√Æn√© sur des millions de paires de phrases pour apprendre :
- Quelles phrases sont similaires
- Quelles phrases sont diff√©rentes
- Comment capturer le contexte et les nuances

---

## üé® Flux complet d'une recherche

```
1. Utilisateur tape : "python data science"
   ‚Üì
2. Frontend Vue envoie : GET /search?q=python data science
   ‚Üì
3. Flask re√ßoit la requ√™te
   ‚Üì
4. Sentence Transformers encode la requ√™te
   "python data science" ‚Üí [0.23, 0.67, -0.12, ..., 0.89]
   ‚Üì
5. FAISS cherche les 5 vecteurs les plus proches
   R√©sultat : [doc_42, doc_15, doc_103, doc_8, doc_67]
   ‚Üì
6. SQLite r√©cup√®re les m√©tadonn√©es
   SELECT * FROM docs WHERE id IN (42, 15, 103, 8, 67)
   ‚Üì
7. Flask construit la r√©ponse JSON
   [{id: 42, title: "...", score: 0.12}, ...]
   ‚Üì
8. Frontend affiche les r√©sultats
```

---

## üß™ Exemple concret

### Documents index√©s :
```
[1] "Python est un langage de programmation"
[2] "JavaScript est utilis√© pour le web"
[3] "Le machine learning utilise Python"
[4] "Les pandas mangent du bambou"
```

### Requ√™te : "programmation python"

**√âtape 1 : Encodage**
```python
query_vector = model.encode("programmation python")
# ‚Üí [0.45, 0.23, -0.67, ..., 0.12]
```

**√âtape 2 : Calcul des distances**
```
Distance avec doc[1] : 0.15  ‚Üê Tr√®s proche !
Distance avec doc[2] : 0.89
Distance avec doc[3] : 0.32  ‚Üê Assez proche
Distance avec doc[4] : 1.45  ‚Üê Tr√®s diff√©rent
```

**√âtape 3 : R√©sultats (tri√©s par distance)**
```json
[
  {
    "id": 1,
    "title": "Python est un langage de programmation",
    "score": 0.15
  },
  {
    "id": 3,
    "title": "Le machine learning utilise Python",
    "score": 0.32
  }
]
```

**Remarque :** Le document [4] sur les pandas (animaux) n'est pas retourn√© car il est trop diff√©rent, m√™me s'il contient le mot "pandas" qui existe aussi en Python !

---

## üîß API Endpoints

### `GET /`
Informations sur l'index

**R√©ponse :**
```json
{
  "message": "FAISS API ready",
  "index_size": 1000,
  "dimension": 384
}
```

### `GET /search?q=votre requ√™te`
Recherche s√©mantique

**Param√®tres :**
- `q` : Texte de la requ√™te (requis)

**R√©ponse :**
```json
[
  {
    "id": 42,
    "title": "Introduction to Machine Learning",
    "snippet": "Machine learning is a subset...",
    "score": 0.12
  }
]
```

**Note :** Le score est une distance. **Plus il est bas, meilleur est le r√©sultat.**

---

## üìñ Ressources

### Documentation
- [FAISS](https://faiss.ai/) - Biblioth√®que de recherche vectorielle
- [Sentence Transformers](https://www.sbert.net/) - Mod√®les d'embeddings
- [Flask](https://flask.palletsprojects.com/) - Framework web Python

### Mod√®les alternatifs
Explorer sur [Hugging Face](https://huggingface.co/models?library=sentence-transformers) pour trouver des mod√®les adapt√©s √† votre cas d'usage sp√©cifique.

---

**Le principe fondamental :** Transformer du texte en nombres, puis utiliser les math√©matiques pour trouver ce qui est similaire. Simple et puissant ! üöÄ