# Semantic Search Engine - Mon premier projet IA avec Python

## ğŸ“‹ Vue d'ensemble

Application de **recherche sÃ©mantique** qui comprend le sens des requÃªtes plutÃ´t que de faire une simple correspondance de mots-clÃ©s. Par exemple, chercher "voiture rapide" trouvera aussi des documents contenant "automobile sportive" ou "vÃ©hicule performant".

Ce projet a Ã©tÃ© dÃ©veloppÃ© pour apprendre et comprendre comment fonctionne un moteur de recherche moderne et a par la mÃªme occasion servi de projet final soumis Ã  Harvard pour valider mon cours CS50.


## ğŸ“š Table des matiÃ¨res

- [ğŸ“‹ Vue d'ensemble](#-vue-densemble)
- [ğŸš€ DÃ©marrage](#-dÃ©marrage)
- [ğŸ§  Comment fonctionne la recherche sÃ©mantique ?](#-comment-fonctionne-la-recherche-sÃ©mantique-)
  - [Le problÃ¨me avec la recherche classique](#le-problÃ¨me-avec-la-recherche-classique)
  - [La solution : les embeddings vectoriels](#la-solution--les-embeddings-vectoriels)
- [ğŸ”¬ Architecture du moteur de recherche](#-architecture-du-moteur-de-recherche)
  - [Ã‰tape 1 : PrÃ©paration des donnÃ©es](#Ã©tape-1--prÃ©paration-des-donnÃ©es-datasetpy)
  - [Ã‰tape 2 : Indexation](#Ã©tape-2--indexation-indexerpy)
  - [Ã‰tape 3 : Recherche en temps rÃ©el](#Ã©tape-3--recherche-en-temps-rÃ©el-apppy)
- [ğŸ¯ Pourquoi c'est puissant ?](#-pourquoi-cest-puissant-)
- [ğŸ” FAISS : Le cÅ“ur du systÃ¨me](#-faiss--le-cÅ“ur-du-systÃ¨me)
  - [Comparaison des index FAISS](#comparaison-des-index-faiss)
- [ğŸ¨ Flux complet d'une recherche](#-flux-complet-dune-recherche)
- [ğŸ§ª Exemple concret](#-exemple-concret)
- [ğŸ”§ API Endpoints](#-api-endpoints)

--- 

### Technologies principales
- **Backend** : Flask + FAISS + Sentence Transformers
- **Frontend** : Vue.js 3 + Vite
- **Orchestration** : Docker Compose

---

## ğŸš€ DÃ©marrage

```bash
# Cloner et lancer
git clone <repo>
cd semantic-search

cd backend
python dataset.py
python indexer.py

cd ..
docker-compose up --build

# Puis ouvrir dans votre navigateur
# Frontend : http://localhost:5173
# API : http://localhost:5000
```

---

## ğŸ§  Comment fonctionne la recherche sÃ©mantique ?

### Le problÃ¨me avec la recherche classique

Une recherche traditionnelle cherche des **correspondances exactes** :
- RequÃªte : "voiture rapide"
- Trouve : documents contenant exactement "voiture" ET "rapide"
- Rate : "automobile sportive", "vÃ©hicule performant", "bolide"

### La solution : les embeddings vectoriels

Au lieu de comparer des mots, on compare des **vecteurs mathÃ©matiques** qui reprÃ©sentent le **sens** du texte.

```
"machine learning"          â†’  [0.2, 0.8, 0.1, ..., 0.5]  (384 dimensions)
"apprentissage automatique" â†’  [0.21, 0.79, 0.11, ..., 0.49]

SimilaritÃ© cosinus Ã©levÃ©e â†’ textes similaires !
```

---

## ğŸ”¬ Architecture du moteur de recherche

### Ã‰tape 1 : PrÃ©paration des donnÃ©es (`dataset.py`)

```python
documents = [
    "Machine learning is a subset of AI",
    "Deep learning uses neural networks",
    "Python is great for data science"
]
```

GÃ©nÃ¨re :
- **`data/`** : Fichiers sources bruts
- **`docs.db`** : Base SQLite avec mÃ©tadonnÃ©es (id, titre, texte)

### Ã‰tape 2 : Indexation (`indexer.py`)

```python
# 1. Charger le modÃ¨le
model = SentenceTransformer("all-MiniLM-L6-v2")

# 2. Encoder tous les documents
embeddings = model.encode(documents)
# â†’ Matrice de vecteurs normalisÃ©s pour la similaritÃ© cosinus

# 3. CrÃ©er l'index FAISS avec similaritÃ© cosinus
index = faiss.IndexFlatIP(384)  # IP = Inner Product (cosinus sur vecteurs normalisÃ©s)

# 4. Ajouter les vecteurs
index.add(embeddings)

# 5. Sauvegarder
faiss.write_index(index, "indexes.faiss")
```

**RÃ©sultat :** `indexes.faiss` - Index optimisÃ© pour la recherche rapide

### Ã‰tape 3 : Recherche en temps rÃ©el (`app.py`)

```python
# 1. Utilisateur cherche : "deep learning"
query_vector = model.encode(query)

# 2. Chercher les 5 vecteurs les plus similaires
similarities, indices = index.search(query_vector, k=5)

# similarities = [0.95, 0.87, 0.72, 0.65, 0.58]  # Plus Ã©levÃ© = plus similaire
# indices = [42, 15, 103, 8, 67]

# 3. RÃ©cupÃ©rer les documents depuis SQLite
results = [
    {"id": 42, "title": "...", "score": 0.95},
    {"id": 15, "title": "...", "score": 0.87},
    ...
]
```

---

## ğŸ¯ Pourquoi c'est puissant ?

### 1. Comprend les synonymes
```
RequÃªte : "automobile"
Trouve : "voiture", "vÃ©hicule", "car"
```

### 2. Comprend le contexte
```
RequÃªte : "apple fruit"
Trouve : documents sur les pommes (fruit)
Pas : documents sur Apple (entreprise)
```

### 3. Fonctionne en multilingue
```
RequÃªte : "machine learning"
Trouve aussi : "apprentissage automatique"
```

### 4. Rapide et scalable
- **10 000 documents** : ~5-10ms
- **1 million de documents** : ~50-100ms

---

## ğŸ” FAISS : Le cÅ“ur du systÃ¨me

**FAISS** (Facebook AI Similarity Search) permet la recherche vectorielle ultra-rapide.

### IndexFlatIP vs IndexFlatL2

Notre projet utilise `IndexFlatIP` qui calcule le **produit scalaire** :

```python
index = faiss.IndexFlatIP(384)
```

**Quand utiliser IndexFlatIP :**
- **Vecteurs normalisÃ©s** (comme Sentence Transformers) : Ã©quivalent Ã  la similaritÃ© cosinus
- **Score intuitif** : valeurs 0-1, plus Ã©levÃ© = meilleur
- **Recherche sÃ©mantique** : on compare l'orientation (le sens), pas la magnitude
- **Recommandations** : "cet article est similaire Ã  celui-ci"

**Quand utiliser IndexFlatL2 :**
- **Vecteurs non normalisÃ©s** oÃ¹ la magnitude compte
- **Embeddings d'images** avec des modÃ¨les qui ne normalisent pas
- **Distance physique** : coordonnÃ©es GPS, donnÃ©es spatiales
- **DonnÃ©es numÃ©riques brutes** : tempÃ©rature, prix, mesures

**Note importante :** Avec Sentence Transformers, les vecteurs sont automatiquement normalisÃ©s. Dans ce cas, **IndexFlatIP et IndexFlatL2 donnent des classements identiques** (seule l'Ã©chelle des scores diffÃ¨re). On prÃ©fÃ¨re IP pour l'interprÃ©tabilitÃ© du score.

### Comparaison des index FAISS

| Index | Vitesse | PrÃ©cision | RAM | Meilleur pour |
|-------|---------|-----------|-----|---------------|
| **FlatIP/L2** | Lent (O(n)) | 100% | Ã‰levÃ©e | < 100k vecteurs, prÃ©cision critique |
| **IVFFlat** | Rapide (O(log n)) | 90-95% | Moyenne | 100k-10M vecteurs, bon Ã©quilibre |
| **IVFPQ** | TrÃ¨s rapide | 85-90% | Faible | 10M+ vecteurs, RAM limitÃ©e |
| **HNSW** | TrÃ¨s rapide | 95-99% | Ã‰levÃ©e | Meilleure qualitÃ© approximative |

**Notre choix :** IndexFlatIP car prÃ©cision maximale pour un dataset de taille modÃ©rÃ©e (~1k-100k documents).

---

## ğŸ“Š Sentence Transformers : Le cerveau

### Le modÃ¨le : all-MiniLM-L6-v2

**CaractÃ©ristiques :**
- **Dimension** : 384
- **Taille** : ~80 MB
- **Vitesse** : ~14 000 phrases/seconde (CPU)
- **QualitÃ©** : Excellent Ã©quilibre performance/prÃ©cision

```python
model = SentenceTransformer("all-MiniLM-L6-v2")

model.encode("chat")  # â†’ [0.12, -0.54, ...]
model.encode("J'aime les chats")  # â†’ [0.15, -0.52, ...]
```

Le modÃ¨le a Ã©tÃ© entraÃ®nÃ© sur des millions de paires de phrases pour capturer le sens sÃ©mantique.

---

## ğŸ¨ Flux complet d'une recherche

```
1. Utilisateur : "python data science"
   â†“
2. Frontend â†’ GET /search?q=python data science
   â†“
3. Flask encode la requÃªte
   â†’ [0.23, 0.67, -0.12, ..., 0.89]
   â†“
4. FAISS calcule les similaritÃ©s cosinus
   â†’ [doc_42: 0.89, doc_15: 0.82, doc_103: 0.75]
   â†“
5. SQLite rÃ©cupÃ¨re les mÃ©tadonnÃ©es
   â†“
6. JSON â†’ Frontend
   [{id: 42, title: "...", score: 0.89}, ...]
```

---

## ğŸ§ª Exemple concret

### Documents indexÃ©s :
```
[1] "Python est un langage de programmation"
[2] "JavaScript est utilisÃ© pour le web"
[3] "Le machine learning utilise Python"
[4] "Les pandas mangent du bambou"
```

### RequÃªte : "programmation python"

**SimilaritÃ©s cosinus calculÃ©es :**
```
doc[1] : 0.92  â† TrÃ¨s similaire !
doc[2] : 0.35
doc[3] : 0.78  â† Assez similaire
doc[4] : 0.12  â† Peu similaire
```

**RÃ©sultats (triÃ©s par score dÃ©croissant) :**
```json
[
  {
    "id": 1,
    "title": "Python est un langage de programmation",
    "score": 0.92
  },
  {
    "id": 3,
    "title": "Le machine learning utilise Python",
    "score": 0.78
  }
]
```

---

## ğŸ”§ API Endpoints

### `GET /`
Informations sur l'index

```json
{
  "message": "FAISS API ready",
  "index_size": 1000,
  "dimension": 384
}
```

### `GET /search?q=votre requÃªte`
Recherche sÃ©mantique

**RÃ©ponse :**
```json
[
  {
    "id": 42,
    "title": "Introduction to Machine Learning",
    "snippet": "Machine learning is a subset...",
    "score": 0.89
  }
]
```

**Note :** Avec `IndexFlatIP`, **plus le score est Ã©levÃ©, meilleur est le rÃ©sultat** (contrairement Ã  L2 oÃ¹ un score bas est meilleur).

---

## ğŸ“– Ressources

- [FAISS](https://faiss.ai/) - BibliothÃ¨que de recherche vectorielle
- [Sentence Transformers](https://www.sbert.net/) - ModÃ¨les d'embeddings
- [Flask](https://flask.palletsprojects.com/) - Framework web Python
- [Hugging Face Models](https://huggingface.co/models?library=sentence-transformers) - ModÃ¨les alternatifs

---

**Le principe :** Transformer du texte en vecteurs, calculer leur similaritÃ© cosinus pour trouver ce qui est sÃ©mantiquement proche. Simple et puissant ! ğŸš€
