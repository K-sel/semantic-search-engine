# Programme d'extraction de dataset Hugging face Ã©crit par intelligence artificielle#
# But : Sortir 100 fichiers textes et les mettre dans ./data

from datasets import load_dataset
import os


# CrÃ©e le dossier si pas existant
os.makedirs("data", exist_ok=True)

# Charge le dataset en streaming
ds = load_dataset("debasishraychawdhuri/wikipedia_clean_5GB", split="train", streaming=True)

# Variables de contrÃ´le
target_docs = 1000  # Nombre de documents voulus
saved_count = 0
processed_count = 0
min_length = 500  # Longueur minimale en caractÃ¨res

print(f"ğŸ” Recherche de {target_docs} documents Wikipedia avec au moins {min_length} caractÃ¨res...")

# ItÃ¨re et sauvegarde seulement les documents non-vides
for item in ds:
    processed_count += 1
    
    # VÃ©rifie si le texte existe et n'est pas vide
    if 'text' in item and item['text'] and len(item['text'].strip()) >= min_length:
        filename = f"data/wiki_{saved_count}.txt"
        
        with open(filename, "w", encoding="utf-8") as f:
            f.write(item['text'])
        
        # Affiche un aperÃ§u du document sauvegardÃ©
        preview = item['text'][:100].replace('\n', ' ')
        print(f"âœ… Document {saved_count}: {len(item['text'])} chars - {preview}...")
        
        saved_count += 1
        
        # ArrÃªte quand on a assez de documents
        if saved_count >= target_docs:
            break
    else:
        # Debug : affiche pourquoi le document est rejetÃ©
        text_len = len(item.get('text', '').strip()) if item.get('text') else 0
        print(f"âŒ Document {processed_count} rejetÃ©: {text_len} chars (trop court)")

print(f"\nğŸ‰ TerminÃ© ! {saved_count} documents sauvegardÃ©s sur {processed_count} traitÃ©s.")

# VÃ©rification finale
print("\nğŸ“Š RÃ©sumÃ© des fichiers crÃ©Ã©s:")
for i in range(saved_count):
    filename = f"data/wiki_{i}.txt"
    if os.path.exists(filename):
        size = os.path.getsize(filename)
        print(f"  - {filename}: {size} bytes")


